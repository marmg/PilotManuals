{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get text from documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/hftest/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/opt/miniconda3/envs/hftest/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from haystack.nodes import PreProcessor\n",
    "from haystack.utils import convert_files_to_dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use haystack utils to read pdf documents and convert them to the dict convention of haystack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.utils.preprocessing -  Converting ../data/aerospace/7110.10BB_Basic_w_Chg_1_dtd_12-2-21.pdf\n",
      "INFO - haystack.utils.preprocessing -  Converting ../data/aerospace/7110.65Z_ATC_Bsc_w_Chg_1_dtd_12-2-21.pdf\n"
     ]
    }
   ],
   "source": [
    "all_docs = convert_files_to_dicts(dir_path=\"../data/aerospace/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although Haystack provides a processor useful enough for most of the documents, to obtain production results a specific and dedicated preprocessing of the documents is usually needed.\n",
    "\n",
    "Therefore, to understand documents and variability of them is mandatory to process them successfuly.\n",
    "\n",
    "In this case, documents are structured always as follows:\n",
    "    - First, a history of changes.\n",
    "    - Secondly, table of contents.\n",
    "    - Finally, the content.\n",
    "\n",
    "Chapter and sections have the same naming convention, so it is easy to split them acording to subsections and save chapter and section as document metadata, which may help in the future to filter documents. \n",
    "    - Chapters always start with <Chapter>, followed by the number of the chapter and the name. They don't end with a fullstop.\n",
    "    - Sections always start with <Section>, followed by the number of the section and the name. They don't end with a fullstop.\n",
    "    - Subsections always start with the number of the subsection, which is: <chapter number>-<section number>-<subsection number>, followed by the name of the subsection in uppercase.\n",
    "\n",
    "Also, headers and footers are well defined. Headers contain the filename and the date, and the footers always have section name and subsection number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for doc in all_docs:\n",
    "    name = doc['meta']['name']\n",
    "    content = doc['content']\n",
    "    # Remove the history of changes and the table of contents by starting at the chapter 1\n",
    "    doc_init = re.search(\"(?<!Table of Contents\\s)Chapter 1\\. General\", content).span()[0]\n",
    "    content = content[doc_init:]\n",
    "    new_content = \"\"\n",
    "    chapter = \"\"\n",
    "    chapter_number = \"\"\n",
    "    section = \"\"\n",
    "    section_number = \"\"\n",
    "    subsection = \"\"\n",
    "    subsection_number = \"\"\n",
    "    for line in content.split(\"\\n\"):\n",
    "        line = re.sub(\" +\", \" \", line)  # Replace more than one space by only one\n",
    "        # Check if line is a chapter declaration\n",
    "        res_pat_chapter = re.findall(\"Chapter (\\d)\\. ([A-Za-z\\s]*)\", line)\n",
    "        if res_pat_chapter and line[-1] != \".\":\n",
    "            chapter_tmp = res_pat_chapter[0][1]\n",
    "            chapter_number_tmp = res_pat_chapter[0][0]\n",
    "        # Check if line is a section declaration\n",
    "        res_pat_section = re.findall(\"Section (\\d)\\. ([A-Za-z\\s]*)\", line) \n",
    "        if res_pat_section and line[-1] != \".\":\n",
    "            section_tmp = res_pat_section[0][1]\n",
    "            section_number_tmp = res_pat_section[0][0]\n",
    "        \n",
    "        # Check is line is a subsection declaration. In case it is, store line before as a new document\n",
    "        res_pat = re.findall(\"^\\d[\\-−]\\d[\\-−]\\d\\.\", line)\n",
    "        if (line.isupper() and res_pat) or res_pat_chapter or res_pat_section:\n",
    "            if new_content:\n",
    "                docs.append({\n",
    "                    'content': new_content, \n",
    "                    'meta': {\n",
    "                        'name': name,\n",
    "                        'chapter': chapter,\n",
    "                        'chapter_number': chapter_number,\n",
    "                        'section': section,\n",
    "                        'section_number': section_number,\n",
    "                        'subsection': subsection,\n",
    "                        'subsection_number': subsection_number\n",
    "                    }\n",
    "                })\n",
    "            new_content = line if line.isupper() else \"\"\n",
    "            subsection = line if line.isupper() else subsection\n",
    "            subsection_number = res_pat[0] if res_pat else subsection_number\n",
    "            chapter = chapter_tmp\n",
    "            chapter_number = chapter_number_tmp\n",
    "            section = section_tmp if res_pat_section else section\n",
    "            section_number = section_number_tmp if res_pat_section else section_number\n",
    "        # If it's not a subsection, add the line to the new document content\n",
    "        else:\n",
    "            # Check for headers or footers\n",
    "            is_date = bool(re.findall(\"\\d/\\d/\\d\", line))\n",
    "            is_filename = bool(re.findall(\"[A-Z][2] [0-9][4]\\.[0-9]{,3}[A-Z]{,3}\", line))\n",
    "            is_section_number = bool(re.findall(\"\\d[\\-−]\\d[\\-−]\\d\", line))\n",
    "            if line != section and not is_date and not is_filename and not is_section_number:\n",
    "                new_content += \"\\n\" + line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a generator to take a look at the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_doc():\n",
    "    for doc in docs:\n",
    "        yield doc\n",
    "    \n",
    "gen = gen_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '1-1-1. PURPOSE OF THIS ORDER\\nThis order prescribes procedures and phraseology for\\nuse by air traffic personnel providing flight services.\\nFlight service specialists are required to be familiar\\nwith the provisions of this order that pertain to their\\noperational responsibilities and to exercise their best\\njudgment if they encounter situations that are not\\ncovered.',\n",
       " 'meta': {'name': '7110.10BB_Basic_w_Chg_1_dtd_12-2-21.pdf',\n",
       "  'chapter': 'General',\n",
       "  'chapter_number': '1',\n",
       "  'section': 'Introduction',\n",
       "  'section_number': '1',\n",
       "  'subsection': '1-1-1. PURPOSE OF THIS ORDER',\n",
       "  'subsection_number': '1-1-1.'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store them into ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store documents into an ElasticSearch cluster. This will help to retrieve documents with the QA pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_stores import ElasticsearchDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/hftest/lib/python3.8/site-packages/elasticsearch/connection/base.py:190: ElasticsearchDeprecationWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haystack provides nodes to retrieve documents in different ways: based on TF-IDF, DPR, Tables, Embeddings, etc.\n",
    "\n",
    "A complete understanding of documents and the usecase is needed to make good decissions about which one use.\n",
    "\n",
    "In this case, documents are mostly written in natural language, they have some tables though. \n",
    "Users can make question in different ways, so to use a TF-IDF retriever would not be a good choice as it may fails depending on what words the user uses. \n",
    "The split has been done by subsections, so each document in elastic can have a lot of text, not like questions which will often be short.\n",
    "Therefore, DPR is the best choice, as it will encode query and context in a different way, in order to get the best matches.\n",
    "\n",
    "Next steps:\n",
    "    - As there are some tables in the documents, to detect and parse them would be a good idea to find information within them, using the TableRetriever\n",
    "    - To use a NER to save named entities and to have an ontology to save entities from it would be also of interest to filter documents or to train a TF-IDF retriever based on these entities.\n",
    "    - As it is a complex task, to make users to select some features (like section, plane model, etc) would help a lot\n",
    "    - As always, to finetune a model over these documents would improve language models performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import DensePassageRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA:0\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find facebook/dpr-question_encoder-single-nq-base locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded facebook/dpr-question_encoder-single-nq-base\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find facebook/dpr-ctx_encoder-single-nq-base locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded facebook/dpr-ctx_encoder-single-nq-base\n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(document_store=document_store, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/hftest/lib/python3.8/site-packages/elasticsearch/connection/base.py:190: ElasticsearchDeprecationWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n",
      "INFO - haystack.document_stores.elasticsearch -  Updating embeddings for all 1023 docs ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da8db3109dc457db8a4cc5050e1caee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Updating embeddings:   0%|          | 0/1023 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/1024 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Update embeddings for DPR\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load QA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n",
      "INFO - haystack.modeling.model.language_model -  LOADING MODEL\n",
      "INFO - haystack.modeling.model.language_model -  =============\n",
      "INFO - haystack.modeling.model.language_model -  Could not find deepset/roberta-base-squad2 locally.\n",
      "INFO - haystack.modeling.model.language_model -  Looking on Transformers Model Hub (in local cache and online)...\n",
      "INFO - haystack.modeling.model.language_model -  Loaded deepset/roberta-base-squad2\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='prediction_heads' was already logged with value='TextSimilarityHead' for run ID='6f204ea64e81433791bf3d05699cd107'. Attempted logging new value 'QuestionAnsweringHead'.\n",
      "WARNING - haystack.modeling.logger -  Failed to log params: Changing param values is not allowed. Param with key='processor' was already logged with value='TextSimilarityProcessor' for run ID='6f204ea64e81433791bf3d05699cd107'. Attempted logging new value 'SquadProcessor'.\n",
      "INFO - haystack.modeling.logger -  ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n",
      "INFO - haystack.modeling.utils -  Using devices: CUDA\n",
      "INFO - haystack.modeling.utils -  Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Use a standard model for Extractive QA\n",
    "# It would be better to finetune a model over these documents, but for a demo is enough\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True, num_processes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the QA Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/hftest/lib/python3.8/site-packages/elasticsearch/connection/base.py:190: ElasticsearchDeprecationWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.58 Batches/s]\n"
     ]
    }
   ],
   "source": [
    "question = \"What should operational systems record?\"\n",
    "# Document: JO 7110.10BB \n",
    "# Subsection: 2−1−8.LOGGING PILOT BRIEFINGS \n",
    "# the facility/sector, date, position, time, and specialist identification for each logged briefing\n",
    "prediction = pipe.run(\n",
    "    query=question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the facility/sector, date,\\nposition, time, and specialist identification',\n",
       "  {'chapter': 'Pilot Briefing',\n",
       "   'subsection': '2-1-8. LOGGING PILOT BRIEFINGS',\n",
       "   'subsection_number': '2-1-8.',\n",
       "   'chapter_number': '2',\n",
       "   'section': 'General',\n",
       "   'section_number': '1',\n",
       "   'name': '7110.10BB_Basic_w_Chg_1_dtd_12-2-21.pdf'}),\n",
       " ('Aircraft contact information',\n",
       "  {'chapter': 'ICAO designators and',\n",
       "   'subsection': '5-1-5. METHODS OF RECORDING DATA',\n",
       "   'subsection_number': '5-1-5.',\n",
       "   'chapter_number': '5',\n",
       "   'section': 'General',\n",
       "   'section_number': '1',\n",
       "   'name': '7110.10BB_Basic_w_Chg_1_dtd_12-2-21.pdf'}),\n",
       " ('Aircraft contact information',\n",
       "  {'chapter': 'Inflight Services',\n",
       "   'subsection': '3-2-2. METHODS OF RECORDING DATA',\n",
       "   'subsection_number': '3-2-2.',\n",
       "   'chapter_number': '3',\n",
       "   'section': 'Data Recording',\n",
       "   'section_number': '2',\n",
       "   'name': '7110.10BB_Basic_w_Chg_1_dtd_12-2-21.pdf'}),\n",
       " ('Flight plan information',\n",
       "  {'chapter': 'Inflight Services',\n",
       "   'subsection': '3-2-2. METHODS OF RECORDING DATA',\n",
       "   'subsection_number': '3-2-2.',\n",
       "   'chapter_number': '3',\n",
       "   'section': 'Data Recording',\n",
       "   'section_number': '2',\n",
       "   'name': '7110.10BB_Basic_w_Chg_1_dtd_12-2-21.pdf'}),\n",
       " ('notify rescue\\ncoordination centers immediately when an aircraft is\\nconsidered to be in a state of emergency',\n",
       "  {'chapter': 'ICAO designators and',\n",
       "   'subsection': '6-3-2. ALERTING PHASES',\n",
       "   'subsection_number': '6-3-2.',\n",
       "   'chapter_number': '5',\n",
       "   'section': 'Alerting Service',\n",
       "   'section_number': '3',\n",
       "   'name': '7110.10BB_Basic_w_Chg_1_dtd_12-2-21.pdf'}),\n",
       " ('news of the aircraft',\n",
       "  {'chapter': 'International Operations',\n",
       "   'subsection': '6-3-2. ALERTING PHASES',\n",
       "   'subsection_number': '6-3-2.',\n",
       "   'chapter_number': '6',\n",
       "   'section': 'Alerting Service',\n",
       "   'section_number': '3',\n",
       "   'name': '7110.10BB_Basic_w_Chg_1_dtd_12-2-21.pdf'}),\n",
       " ('Collect pertinent information regarding the\\nincident.\\nd. Notify the operations supervisor of the false,\\ndeceptive, or phantom transmission and report all\\nrelevant information pertaining to the incident',\n",
       "  {'chapter': 'General Control',\n",
       "   'subsection': '2-4-6. FALSE OR DECEPTIVE',\n",
       "   'subsection_number': '2-4-6.',\n",
       "   'chapter_number': '2',\n",
       "   'section': 'Radio and Interphone Communications',\n",
       "   'section_number': '4',\n",
       "   'name': '7110.65Z_ATC_Bsc_w_Chg_1_dtd_12-2-21.pdf'}),\n",
       " ('Aircraft altitude',\n",
       "  {'chapter': 'Emergency Services',\n",
       "   'subsection': '4-2-1. INFORMATION REQUIREMENTS',\n",
       "   'subsection_number': '4-2-1.',\n",
       "   'chapter_number': '4',\n",
       "   'section': 'Operations',\n",
       "   'section_number': '2',\n",
       "   'name': '7110.10BB_Basic_w_Chg_1_dtd_12-2-21.pdf'}),\n",
       " ('Terrain/Obstruction Alert',\n",
       "  {'chapter': 'General Control',\n",
       "   'subsection': '2-1-5. EXPEDITIOUS COMPLIANCE',\n",
       "   'subsection_number': '2-1-5.',\n",
       "   'chapter_number': '2',\n",
       "   'section': 'General',\n",
       "   'section_number': '1',\n",
       "   'name': '7110.65Z_ATC_Bsc_w_Chg_1_dtd_12-2-21.pdf'}),\n",
       " ('pilot briefing\\nrecords',\n",
       "  {'chapter': 'Pilot Briefing',\n",
       "   'subsection': '2-1-8. LOGGING PILOT BRIEFINGS',\n",
       "   'subsection_number': '2-1-8.',\n",
       "   'chapter_number': '2',\n",
       "   'section': 'General',\n",
       "   'section_number': '1',\n",
       "   'name': '7110.10BB_Basic_w_Chg_1_dtd_12-2-21.pdf'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(answer.answer, answer.meta) for answer in prediction['answers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for answer in prediction['answers'][:10]:\n",
    "    answers.append({\n",
    "        'answer': answer.answer,\n",
    "        'chapter': f\"Chapter {answer.meta['chapter_number']}: {answer.meta['chapter']}\",\n",
    "        'section': f\"Section {answer.meta['section_number']}: {answer.meta['section']}\",\n",
    "        'subsection': f\"subsection {answer.meta['subsection_number']}: {answer.meta['subsection']}\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hftest] *",
   "language": "python",
   "name": "conda-env-hftest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
